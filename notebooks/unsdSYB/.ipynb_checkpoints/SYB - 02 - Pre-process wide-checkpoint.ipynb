{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing script 2\n",
    "- pivot into \"wide\" format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import urllib3  # allows to access a URL with python\n",
    "import math\n",
    "import os\n",
    "import io\n",
    "import collections\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import xlsxwriter\n",
    "\n",
    "# https://volderette.de/jupyter-notebook-tip-multiple-outputs/\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir_path = os.path.dirname(os.path.realpath('__file__'))\n",
    "print(dir_path)\n",
    "\n",
    "data_dir = r'../../data/unsd/UNSYB/input/'\n",
    "print('data inputs dir: ' + data_dir)\n",
    "\n",
    "output_dir = r'../../data/unsd/UNSYB/output/'\n",
    "print('outputs dir: ' + output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert string to camelCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camelCase(st):\n",
    "    \"\"\"\n",
    "    https://stackoverflow.com/questions/8347048/camelcase-every-string-any-standard-library\n",
    "    \n",
    "    \"\"\"\n",
    "    output = ''.join(x for x in st.title() if x.isalnum())\n",
    "    return output[0].lower() + output[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disable insecure request warnings when using `urllib3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular expression to capture numeric values (including those in scientific notation)\n",
    "The regex is\n",
    "\n",
    "```\n",
    "-?      # an optional -\n",
    "\\d+     # a series of digits\n",
    "(?:     # start non capturing group\n",
    "  \\.    # a dot\n",
    "  \\d+   # a series of digits\n",
    ")?      \n",
    "(?:     # start non capturing group\n",
    "  e     # \"e\"\n",
    "  -?    # an optional -\n",
    "  \\d+   # digits\n",
    ")?\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeric_part(v):\n",
    "    numeric_part_f = re.compile(r'-?\\d+(?:\\.\\d+)?(?:e-?\\d+)?')\n",
    "    x = numeric_part_f.findall(v)\n",
    "    if len(x) > 0:\n",
    "        return float(x[0])\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute a hash of a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_hash(d):\n",
    "    out = hashlib.md5()\n",
    "    for key, value in d.items():\n",
    "        out.update(key.encode('utf-8'))\n",
    "        out.update(str(value).encode('utf-8'))\n",
    "    return out.hexdigest()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get unique dictionaries in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_dicts(dictionary_list):\n",
    "\n",
    "    uniques_map = {}\n",
    "\n",
    "    for d in dictionary_list:\n",
    "        uniques_map[dict_hash(d)] = d\n",
    "\n",
    "    return list(uniques_map.values())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract subset of key-value pairs from Python dictionary object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subdict_list(dict_list, keys_list, exclude = False):\n",
    "    sub_d_list = []\n",
    "    if exclude:\n",
    "        for d in dict_list:\n",
    "            sub_d= {k: d[k] for k in d.keys() if k not in keys_list}\n",
    "            sub_d_list.append(sub_d)\n",
    "    else:\n",
    "        for d in dict_list:\n",
    "            if set(keys_list) <= set(d.keys()):\n",
    "                sub_d= {k: d[k] for k in keys_list}\n",
    "                sub_d_list.append(sub_d)\n",
    "    \n",
    "    return sub_d_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a dict from a list based on something inside the dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_dict(dict_list, k, v):\n",
    "    selected = []\n",
    "    for d in dict_list:\n",
    "        if k in set(d.keys()):\n",
    "            if d[k] == v:\n",
    "                selected.append(d)\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the coverage of an (unordered) list of years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def year_intervals (years_list):\n",
    "    \"\"\" Find the coverage of an ordered list of years\"\"\"\n",
    "    \n",
    "    years_list = list(map(int, years_list))\n",
    "    \n",
    "    years_list.sort()\n",
    "    \n",
    "    n = len(years_list)\n",
    "    \n",
    "    start_y = list()\n",
    "    end_y = list()\n",
    "    \n",
    "    start_y.append(years_list[0])\n",
    "    \n",
    "    if n > 1:\n",
    "        for i in range(n-1):\n",
    "            if(years_list[i+1] - years_list[i]>1):\n",
    "                start_y.append(years_list[i+1])\n",
    "                end_y.append(years_list[i])\n",
    "    \n",
    "    end_y.append(years_list[n-1])\n",
    "    \n",
    "    interval_yy = list()\n",
    "    \n",
    "    for i in range(len(start_y)):\n",
    "\n",
    "        if  end_y[i] - start_y[i]> 0 :\n",
    "            interval_yy.append(str(start_y[i]) + '-' + str(end_y[i]))\n",
    "        else:\n",
    "            interval_yy.append(str(start_y[i]))\n",
    "\n",
    "    \n",
    "    x = \",\".join(interval_yy)\n",
    "    return(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_intervals(['1995','2000', '1996', '2001','2002','2003','2004'])\n",
    "year_intervals(['1995'])\n",
    "year_intervals(['2000','2004'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = '02'\n",
    "table = '008'\n",
    "series = 'SYB011'\n",
    "\n",
    "file = 'Topic'+topic+'_Table'+table+'_'+series+'.json'\n",
    "\n",
    "with open(output_dir + file) as json_file:  \n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_file(topic, table, series, dir_path):\n",
    "    file = 'Topic'+topic+'_Table'+table+'_'+series+'.json'\n",
    "    with open(dir_path + file) as json_file:  \n",
    "        data = json.load(json_file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data_file(topic = '02', table = '008', series = 'SYB011', dir_path = output_dir)\n",
    "data.keys()\n",
    "data['newSeriesName']\n",
    "data['refAreas'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of countries to be plotted on a map (with XY coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countryListXY(file):\n",
    "    \n",
    "    countryListXY = []\n",
    "    \n",
    "    with open(file, newline = '', encoding='latin-1') as countryList:                                                                                          \n",
    "        countryList = csv.DictReader(countryList, delimiter='\\t')\n",
    "        for row in countryList:\n",
    "            countryListXY.append(dict(row))\n",
    "            \n",
    "    countryListXY = pd.DataFrame(countryListXY).astype({'M49':'str'})\n",
    "    \n",
    "    return(countryListXY)\n",
    "\n",
    "    #print(countryListXY[1])\n",
    "    #for c in countryListXY:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_dictXY = countryListXY('../../globalResources/refAreas.txt').to_dict(orient = 'records')\n",
    "country_dictXY[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add coordinates to data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_coordinates(data, cordinates):\n",
    "data = read_data_file(topic = '02', table = '008', series = 'SYB011', dir_path = output_dir)\n",
    "coordinates = countryListXY('../../globalResources/refAreas.txt').to_dict(orient = 'records')\n",
    "\n",
    "for g in data['refAreas']:\n",
    "        \n",
    "    g['countryProfile']=None\n",
    "    g['ISO3'] = None\n",
    "    g['UN_Member'] = None\n",
    "    g['X'] = None\n",
    "    g['Y'] = None\n",
    "    \n",
    "    for xy in coordinates:\n",
    "\n",
    "        if xy['M49'].zfill(3) != str(g['refAreaCode']).zfill(3):\n",
    "            continue\n",
    "\n",
    "        g['countryProfile']=xy['Country_Profile']\n",
    "        g['ISO3'] = xy['ISO3']\n",
    "        g['UN_Member'] = xy['UN_Member']\n",
    "        g['X'] = xy['X']\n",
    "        g['Y'] = xy['Y']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select `refAreas` that have coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_areas_publish = []\n",
    "for d in data['refAreas']:\n",
    "    if d['X'] and d['Y']:\n",
    "        ref_areas_publish.append(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_areas_publish[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select distinct years among all data records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "timePeriods = []\n",
    "\n",
    "for ra in ref_areas_publish:\n",
    "    temp.extend(subdict_list(ra['data'], ['year'], exclude = False))\n",
    "    \n",
    "for i in unique_dicts(temp):\n",
    "    timePeriods.append(i['year'])\n",
    "\n",
    "timePeriods.sort()\n",
    "timePeriods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select distinct slices among all data records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "\n",
    "for ra in ref_areas_publish:\n",
    "    temp.extend(subdict_list(ra['data'], ['year', 'value', 'sourceNameEN', 'sourceNameFR', 'footnoteTextEN', 'footnoteTextFR'\n",
    "                                         ], exclude = True))\n",
    "    \n",
    "slices = unique_dicts(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write dataset as \"pivot\" version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_dir + 'SYBcatalogue.json') as json_file:  \n",
    "    catalogue = json.load(json_file)\n",
    "catalogue[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_dir + 'SYBcatalogue.json') as json_file:  \n",
    "    catalogue = json.load(json_file)\n",
    "    \n",
    "{'topicId': 2,\n",
    "  'topicNameEN': 'Population and migration',\n",
    "  'topicNameFR': 'Population et migration',\n",
    "  'tables': [{'tableCode': 'lifecbmort',\n",
    "    'tableId': 8,\n",
    "    'tableName': 'Population growth and indicators of fertility and mortality',\n",
    "    'tableNote': None,\n",
    "    'tableStatusId': 3,\n",
    "    'tbBkCode': 'X03 ',\n",
    "\n",
    "for t in catalogue:\n",
    "    for t2 in t['tables']:\n",
    "        for s in t2['series']:\n",
    "\n",
    "            if s['seriesCode'] != 'SYB122':\n",
    "                continue\n",
    "\n",
    "            # Read data file:\n",
    "            data = read_data_file(topic = str(t['topicId']).zfill(2), table = str(t['tableId']).zfill(3), series =  s['seriesCode'], dir_path = output_dir)\n",
    "\n",
    "            # Select only refAreas that have coordinates:\n",
    "\n",
    "            ref_areas_publish = []\n",
    "            for d in data['refAreas']:\n",
    "                if d['X'] and d['Y']:\n",
    "                    ref_areas_publish.append(d)\n",
    "\n",
    "            # Select unique time periods among all records:\n",
    "\n",
    "            temp = []\n",
    "            timePeriods = []\n",
    "\n",
    "            for ra in ref_areas_publish:\n",
    "                temp.extend(subdict_list(ra['data'], ['year'], exclude = False))\n",
    "\n",
    "            for dt in unique_dicts(temp):\n",
    "                timePeriods.append(dt['year'])\n",
    "\n",
    "            timePeriods.sort()\n",
    "\n",
    "            temp = []\n",
    "\n",
    "            # Obtain unique slices:\n",
    "\n",
    "            for ra in ref_areas_publish:\n",
    "                temp.extend(subdict_list(ra['data'], ['year', 'value', 'sourceNameEN', 'sourceNameFR', 'footnoteTextEN', 'footnoteTextFR'], exclude = True))\n",
    "\n",
    "            slices = unique_dicts(temp)\n",
    "\n",
    "            for ra in ref_areas_publish:\n",
    "\n",
    "                data_new = []\n",
    "\n",
    "                for j in slices:\n",
    "\n",
    "                    # Select data corresponding to reference area ra and slice j:\n",
    "\n",
    "                    slice_values = set(j.values())\n",
    "                    slice_keys = set(j.keys())\n",
    "                    slice_data = []\n",
    "\n",
    "                    slice_data_wide = j.copy()\n",
    "\n",
    "                    for record in ra['data']:\n",
    "                        record_values = set(record.values())\n",
    "                        if slice_values <= record_values:\n",
    "                            slice_data.append(record)\n",
    "\n",
    "                    #----------------------------\n",
    "                    slice_footnotesEN = []\n",
    "                    slice_sourcesEN = []\n",
    "                    slice_years = []\n",
    "\n",
    "\n",
    "                    for r in slice_data:\n",
    "                        if 'footnoteTextEN' in r.keys():\n",
    "                            slice_footnotes.append(r['footnoteTextEN'])\n",
    "                        if 'sourceNameEN' in r.keys():\n",
    "                            slice_sources.append(r['sourceNameEN'])\n",
    "                        if 'years' in r.keys():\n",
    "                            slice_years.append(r['year'])\n",
    "\n",
    "                    slice_footnotes = list(set(slice_footnotes))\n",
    "                    slice_sources = list(set(slice_sources))\n",
    "\n",
    "                    max_year = None\n",
    "                    if len(slice_years)>0:\n",
    "                        max_year = max(slice_years)\n",
    "\n",
    "\n",
    "                    #----------------------------\n",
    "\n",
    "                    slice_footnote_join = []\n",
    "                    counter = 0\n",
    "                    for fn in slice_footnotes:\n",
    "                        counter += 1\n",
    "                        fn_years = []\n",
    "                        for r in slice_data:\n",
    "                            if 'footnotes' in r.keys():\n",
    "                                if fn == r['footnotes']:\n",
    "                                    fn_years.append(r['year'])\n",
    "                        if fn:\n",
    "                            if len(slice_footnotes) > 1:\n",
    "                                slice_footnote_join.append('['+year_intervals(fn_years)+']: ' + fn)\n",
    "                            if len(slice_footnotes) == 1:\n",
    "                                slice_footnote_join.append(fn)\n",
    "\n",
    "\n",
    "                    slice_footnote_join.sort()\n",
    "                    slice_footnote_join = ' // '.join(slice_footnote_join)\n",
    "\n",
    "                    slice_data_wide['footnotes'] = slice_footnote_join\n",
    "\n",
    "                    #-----------------------\n",
    "\n",
    "                    slice_sources_join = []\n",
    "                    counter = 0\n",
    "\n",
    "                    for src in slice_sources:\n",
    "                        counter += 1\n",
    "                        src_years = []\n",
    "                        for r in slice_data:\n",
    "                            if src == r['source']:\n",
    "                                src_years.append(r['timePeriod'])\n",
    "\n",
    "                        if src:\n",
    "                            if len(slice_sources) > 1:\n",
    "                                slice_sources_join.append('['+year_intervals(src_years)+']: ' + src)\n",
    "\n",
    "\n",
    "                            if len(slice_sources) == 1:\n",
    "                                slice_sources_join.append(src)\n",
    "\n",
    "                    slice_sources_join.sort()\n",
    "                    slice_sources_join = ' // '.join(slice_sources_join)\n",
    "\n",
    "                    slice_data_wide['sources'] = slice_sources_join\n",
    "\n",
    "                    #------------------------\n",
    "\n",
    "                    s_keys = list(slice_keys)\n",
    "                    s_keys.extend(['footnotes','source'])\n",
    "\n",
    "\n",
    "                    for y in timePeriods:\n",
    "\n",
    "                        slice_data_y = subdict_list(select_dict(slice_data, 'year', y), \n",
    "                                                    s_keys, exclude = True)\n",
    "                        if len(slice_data_y)>0:\n",
    "                            for ry in slice_data_y:\n",
    "\n",
    "                                if 'value_numeric_part' in ry.keys():\n",
    "                                    slice_data_wide['value_'+str(y)] = ry['value_numeric_part']\n",
    "                                else:\n",
    "                                    slice_data_wide['value_'+str(y)] = None\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            slice_data_wide['value_'+str(y)] = None\n",
    "\n",
    "                    if max_year:\n",
    "                        slice_data_wide['value_latest_year'] = slice_data_wide['value_'+str(max_year)]\n",
    "                        slice_data_wide['latest_year'] = max_year\n",
    "                    else:\n",
    "                        slice_data_wide['value_latest_year'] = None\n",
    "                        slice_data_wide['latest_year'] = None\n",
    "\n",
    "\n",
    "\n",
    "                    data_new.append(slice_data_wide)\n",
    "\n",
    "                del ra['data']\n",
    "                ra['data'] = data_new\n",
    "\n",
    "            new_data = {}\n",
    "            new_data['goal'] = data['goal']\n",
    "            new_data['target'] = data['target']\n",
    "            new_data['indicator'] = data['indicator']\n",
    "            new_data['seriesCode'] = data['seriesCode']\n",
    "            new_data['seriesDesc'] = data['seriesDesc']\n",
    "            new_data['release'] = data['release']\n",
    "            new_data['data'] = ref_areas_publish\n",
    "\n",
    "\n",
    "            file_name = 'wide_Indicator_'+i['reference']+'_Series_'+s['code']+'.json'\n",
    "\n",
    "            with open(wd_dir + r'data\\unsd\\2019.Q1.G.03\\\\' + file_name, 'w') as f:\n",
    "                json.dump(new_data, f, indent=4)\n",
    "\n",
    "\n",
    "            print(file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[0]['hex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
